{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Configuration Settings\n",
    "\n",
    "As a first step, we will import the required libraries and will configure values for different parameters that we will be using in the code. Let's first import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following script to set values for different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Neural machine translation models are often based on the seq2seq architecture. The seq2seq architecture is an encoder-decoder architecture which consists of two LSTM networks: the encoder LSTM and the decoder LSTM. The input to the encoder LSTM is the sentence in the original language; the input to the decoder LSTM is the sentence in the translated language with a start-of-sentence token. The output is the actual target sentence with an end-of-sentence token.\n",
    "\n",
    "In our dataset, we do not need to process the input, however, we need to generate two copies of the translated sentence: one with the start-of-sentence token and the other with the end-of-sentence token. Here is the script which does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 20000\n",
      "num samples output: 20000\n",
      "num samples output input: 20000\n"
     ]
    }
   ],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "count = 0\n",
    "for line in open(r'C:\\Users\\Paaras Jamwal\\Dropbox\\My PC (PAARAS)\\Desktop\\Project\\fra.txt', encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output, attrib = line.rstrip().split('\\t')\n",
    "\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now randomly print a sentence from the input_sentences[], output_sentences[], and output_sentences_inputs[] lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop it!\n",
      "Laisse-le tomber ! <eos>\n",
      "<sos> Laisse-le tomber !\n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[111])\n",
    "print(output_sentences[111])\n",
    "print(output_sentences_inputs[111])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Padding\n",
    "The tokenizer class performs two tasks:\n",
    "\n",
    "   It divides a sentence into the corresponding list of word\n",
    "   Then it converts the words to integers\n",
    "For tokenization, the Tokenizer class from the keras.preprocessing.text library can be used.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script is used to tokenize the input sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 3518\n",
      "Length of longest sentence in input: 6\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output sentences can also be tokenized in the same way as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 9546\n",
      "Length of longest sentence in the output: 12\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to pad the input. The reason behind padding the input and the output is that text sentences can be of varying length, however LSTM (the algorithm that we are going to train our model) expects input instances with the same length. Therefore, we need to convert our sentences into fixed-length vectors. One way to do this is via padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In padding, a certain length is defined for a sentence. In our case the length of the longest sentence in the inputs and outputs will be used for padding the input and output sentences, respectively. The longest sentence in the input contains 6 words. For the sentences that contain less than 6 words, zeros will be added in the empty indexes. The following script applies padding to the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (20000, 6)\n",
      "encoder_input_sequences[111]: [  0   0   0   0 340   4]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[111]:\", encoder_input_sequences[111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs[\"drop\"])\n",
    "print(word2idx_inputs[\"it\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, the decoder outputs and the decoder inputs are padded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (20000, 12)\n",
      "decoder_input_sequences[111]: [  2 726 369   5   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[111]:\", decoder_input_sequences[111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "726\n",
      "369\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_outputs[\"<sos>\"])\n",
    "print(word2idx_outputs[\"laisse-le\"])\n",
    "print(word2idx_outputs[\"tomber\"])\n",
    "print(word2idx_outputs[\"!\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vector. Word embeddings techniques such as GloVe and Word2Vec have proven to be extremely efficient for converting words into corresponding dense vectors. The vector size is small and none of the indexes in the vector is actually empty.\n",
    "There are two main differences between single integer representation and word embeddings. With integer reprensentation, a word is represented only with a single integer. With vector representation a word is represented by a vector of 50, 100, 200, or whatever dimensions you like. Hence, word embeddings capture a lot more information about words. Secondly, the single-integer representation doesn't capture the relationships between different words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Let's create word embeddings for the inputs first. To do so, we need to load the GloVe word vectors into memory. We will then create a dictionary where words are the keys and the corresponding vectors are values, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'C:\\Users\\Paaras Jamwal\\Dropbox\\My PC (PAARAS)\\Desktop\\Project\\glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a matrix where the row number will represent the integer value for the word and the columns will correspond to the dimensions of the word. This matrix will contain the word embeddings for the words in our input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first print the word embeddings for the word \"drop\" using the GloVe word embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2934     0.49122    0.021428  -0.0032031 -0.30625   -0.70369\n",
      " -0.02503    0.20014   -0.11357    0.20024   -0.096917   0.24\n",
      " -0.43177    0.12703    0.50062    0.33529   -0.84563    0.23978\n",
      " -0.094769   0.20813    0.85062   -0.14943   -0.10232    0.74019\n",
      "  0.11983   -0.11003   -0.33117   -0.31358   -0.12285   -0.76261\n",
      "  0.29028   -0.0053144 -0.14366    0.14945   -0.25026    0.51785\n",
      "  0.47238   -0.2494     0.069556   0.041659  -0.49759   -0.17819\n",
      " -0.16676   -0.062862   0.55204    0.026228  -0.029453  -0.35108\n",
      " -0.16275   -1.4509     0.52228    0.044831  -0.25485    0.95877\n",
      " -0.11863   -2.002     -0.29194   -0.22581    2.1544     0.17959\n",
      "  0.052697  -0.022561  -0.47654    0.27268    0.15112    0.13092\n",
      "  0.17475   -0.20071    0.12448   -0.05814   -0.12978    0.24495\n",
      " -0.11701    0.22073   -0.061256   0.015408  -0.33155   -0.020674\n",
      "  0.13623    0.24073    0.63338   -0.11291   -0.75424   -0.27938\n",
      " -0.85368   -1.7428     0.2717    -0.23938    0.3652     0.1187\n",
      " -0.28925    0.41028   -0.70196   -0.45214   -0.56019   -0.25353\n",
      "  0.29411    0.13287    0.53376   -0.60402  ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"drop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29339999  0.49122     0.021428   -0.0032031  -0.30625001 -0.70368999\n",
      " -0.02503     0.20014    -0.11357     0.20024    -0.096917    0.23999999\n",
      " -0.43177     0.12703     0.50062001  0.33529001 -0.84562999  0.23977999\n",
      " -0.094769    0.20813     0.85061997 -0.14943001 -0.10232     0.74019003\n",
      "  0.11983    -0.11003    -0.33116999 -0.31358001 -0.12285    -0.76261002\n",
      "  0.29028001 -0.0053144  -0.14365999  0.14945    -0.25026     0.51784998\n",
      "  0.47238001 -0.2494      0.069556    0.041659   -0.49759001 -0.17818999\n",
      " -0.16676    -0.062862    0.55203998  0.026228   -0.029453   -0.35108\n",
      " -0.16275001 -1.45089996  0.52227998  0.044831   -0.25485     0.95876998\n",
      " -0.11863    -2.00200009 -0.29194    -0.22581001  2.15440011  0.17959\n",
      "  0.052697   -0.022561   -0.47654     0.27268001  0.15112001  0.13091999\n",
      "  0.17475    -0.20071     0.12448    -0.05814    -0.12977999  0.24495\n",
      " -0.11701     0.22073001 -0.061256    0.015408   -0.33155    -0.020674\n",
      "  0.13623001  0.24073     0.63338    -0.11291    -0.75423998 -0.27937999\n",
      " -0.85368001 -1.7428      0.27169999 -0.23938     0.36520001  0.1187\n",
      " -0.28924999  0.41027999 -0.70196003 -0.45214    -0.56019002 -0.25353\n",
      "  0.29411     0.13287     0.53376001 -0.60402   ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[340])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word embedding matrix will be used to create the embedding layer for our LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script creates the embedding layer for the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 \n",
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
